{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment #5: A sentence embedder\n",
    "Author: Pierre Nugues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will implement a sentence embedder simplified from Reimers and Gurevych's Sentence-BERT: https://arxiv.org/pdf/1908.10084. S-BERT is written in PyTorch and its code is available from GitHub: https://github.com/UKPLab/sentence-transformers\n",
    "\n",
    "The objectives of the assignment are to:\n",
    "* Write a program to embed sentences\n",
    "* Use neural networks with PyTorch\n",
    "* Write a short report of 2 to 3 pages to describe your program.\n",
    "\n",
    "Note: Should your machine be unable to train a model for the whole dataset, then use only a fraction of the dataset such as 10% or less. For this, use the `MINI_CORPUS` constant. See below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "We saw we can vectorize words using a dense representation. We can extend this to documents. This enables us to store the resulting vectors in databases and then use fast algorithms for paragraph or document comparisons such as Faiss: https://github.com/facebookresearch/faiss\n",
    "\n",
    "There are many document vectorization techniques and models are regularly benchmarked, see: https://huggingface.co/spaces/mteb/leaderboard. See also a list of available vector databases here https://db-engines.com/en/ranking/vector+dbms\n",
    "\n",
    "In this lab, you will program two techniques to vectorize documents into dense vectors. You will first implement a baseline technique and then a toy version of SBERT. SBERT is one of the earliest transformer-based document vectorization algorithm: _Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks_ by Reimers and Gurevych (2019) https://arxiv.org/pdf/1908.10084"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of the algorithms\n",
    "Read the Getting Started paragraph of https://github.com/UKPLab/sentence-transformers for an overview.\n",
    "\n",
    "Read the summary of the SBERT paper as well as Sections 1 and 3, _Introduction_ and _Model_. In the triplet objective function, an anchor is a start sample, the positive sample is close to the anchor, while the negative one is different. Considering a language detector, think of a sentence in Swedish as the anchor. A positive sample would be another sentence in Swedish and a negative one could be a sentence in English.\n",
    "\n",
    "In the _Method and program struture_ section of your report, you will summarize these sections in 10 to 15 lines. Note that a three-way softmax classifier is simply a logistic regression with three classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import regex as re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a reduced dataset for the development of your program with `MINI_CORPUS` set to `True`. Once your program is ready, you can train your model on the whole dataset (if you have the time). Set `MINI_CORPUS` to `False` then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "MINI_CORPUS = True  # Set the value to True when you develop the program\n",
    "MINI_PERCENTAGE = 0.01  # Percentage of the original dataset.\n",
    "# Depending on your machine, you may even use less than 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets: SNLI\n",
    "As dataset, you will use SNLI. SNLI consists of over 500,000 lines with the text of the pairs and their labels. The authors created the dataset by giving volunteers a sentence (the premise) and asking them to write a second sentence (the hypothesis) that is either definitely true\n",
    "(entailment), that might be true (neutral), or that is definitely false (contradiction).\n",
    "\n",
    "Read the dataset description from this URL https://nlp.stanford.edu/projects/snli/ and download it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please adjust the path to fit your machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('../corpus/snli_1.0/snli_1.0_train.jsonl', 'r') as f:\n",
    "    dataset_list = list(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_snli = []\n",
    "for json_str in dataset_list:\n",
    "    dataset_snli += [json.loads(json_str)]\n",
    "    # print(f\"result: {result}\")\n",
    "    # print(isinstance(result, dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sample with an agreement in the annotation. The final annotation is the gold label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'annotator_labels': ['entailment'],\n",
       " 'captionID': '3706019259.jpg#3',\n",
       " 'gold_label': 'entailment',\n",
       " 'pairID': '3706019259.jpg#3r2e',\n",
       " 'sentence1': 'A foreign family is walking along a dirt path next to the water.',\n",
       " 'sentence1_binary_parse': '( ( A ( foreign family ) ) ( ( is ( ( walking ( along ( a ( dirt path ) ) ) ) ( next ( to ( the water ) ) ) ) ) . ) )',\n",
       " 'sentence1_parse': '(ROOT (S (NP (DT A) (JJ foreign) (NN family)) (VP (VBZ is) (VP (VBG walking) (PP (IN along) (NP (DT a) (NN dirt) (NN path))) (ADVP (JJ next) (PP (TO to) (NP (DT the) (NN water)))))) (. .)))',\n",
       " 'sentence2': 'A family of foreigners walks by the water.',\n",
       " 'sentence2_binary_parse': '( ( ( A family ) ( of foreigners ) ) ( ( walks ( by ( the water ) ) ) . ) )',\n",
       " 'sentence2_parse': '(ROOT (S (NP (NP (DT A) (NN family)) (PP (IN of) (NP (NNS foreigners)))) (VP (VBZ walks) (PP (IN by) (NP (DT the) (NN water)))) (. .)))'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_snli[300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sample with no agreement in the annotation. The gold label is `_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'annotator_labels': ['contradiction', 'contradiction', 'neutral', 'neutral'],\n",
       " 'captionID': '2677109430.jpg#2',\n",
       " 'gold_label': '-',\n",
       " 'pairID': '2677109430.jpg#2r1c',\n",
       " 'sentence1': 'A small group of church-goers watch a choir practice.',\n",
       " 'sentence1_binary_parse': '( ( ( A ( small group ) ) ( of church-goers ) ) ( ( watch ( a ( choir practice ) ) ) . ) )',\n",
       " 'sentence1_parse': '(ROOT (S (NP (NP (DT A) (JJ small) (NN group)) (PP (IN of) (NP (NNS church-goers)))) (VP (VBP watch) (NP (DT a) (NN choir) (NN practice))) (. .)))',\n",
       " 'sentence2': 'A choir performs in front of packed crowd.',\n",
       " 'sentence2_binary_parse': '( ( A choir ) ( ( performs ( in ( front ( of ( packed crowd ) ) ) ) ) . ) )',\n",
       " 'sentence2_parse': '(ROOT (S (NP (DT A) (NN choir)) (VP (VBZ performs) (PP (IN in) (NP (NP (NN front)) (PP (IN of) (NP (JJ packed) (NN crowd)))))) (. .)))'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_snli[145]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove all the samples that have no agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_str = []\n",
    "for sample in dataset_snli:\n",
    "    s1 = sample['sentence1']\n",
    "    s2 = sample['sentence2']\n",
    "    label = sample['gold_label']\n",
    "    if label != '-':\n",
    "        dataset_str += [(s1, s2, label)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "549367"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('A person on a horse jumps over a broken down airplane.',\n",
       " 'A person is training his horse for a competition.',\n",
       " 'neutral')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('A person on a horse jumps over a broken down airplane.',\n",
       " 'A person is at a diner, ordering an omelette.',\n",
       " 'contradiction')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_str[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('A person on a horse jumps over a broken down airplane.',\n",
       " 'A person is outdoors, on a horse.',\n",
       " 'entailment')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_str[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MINI_CORPUS:\n",
    "    new_size = int(len(dataset_str) * MINI_PERCENTAGE)\n",
    "    dataset_str = dataset_str[:new_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5493"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets: GloVe\n",
    "\n",
    "You will first implement a baseline, an easy technique that serves as comparison for more elaborate ones. \n",
    "\n",
    "In Sect. 4.1 and Table 1 the authors proposed a baseline technique for computing a semantic\n",
    "textual similarity between two sentences that uses GloVe embeddings. Describe this technique in 5 to 10 lines in the _Method and program struture_ section. You will create a *Baseline* subsection for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe\n",
    "You will use a list of pretrained word embeddings to implement the baseline and GloVe is one such vector lists. GloVe is available in different dimensionalities (50, 100, 200, 300) and vocabulary sizes (400,000 words, 1.2M, 2.4M). \n",
    "\n",
    "Download the GloVe 6B embeddings from https://nlp.stanford.edu/projects/glove/, uncompress it, and keep the `glove.6B.50d.txt` file of 400,000 words with 50-dimensional vectors.\n",
    "\n",
    "Please adjust your path to read the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_file = '../corpus/glove.6B.50d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_embeddings(file):\n",
    "    \"\"\"\n",
    "    Return the embeddings in the from of a dictionary\n",
    "    :param file:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    embeddings = {}\n",
    "    glove = open(file, encoding='utf8')\n",
    "    for line in glove:\n",
    "        values = line.strip().split()\n",
    "        word = values[0]\n",
    "        vector = torch.tensor(\n",
    "            list(map(float, values[1:])), dtype=torch.float32)\n",
    "        embeddings[word] = vector\n",
    "    glove.close()\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = read_embeddings(embedding_file)\n",
    "embedded_words = sorted(list(embeddings.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4.1800e-01,  2.4968e-01, -4.1242e-01,  1.2170e-01,  3.4527e-01,\n",
       "        -4.4457e-02, -4.9688e-01, -1.7862e-01, -6.6023e-04, -6.5660e-01,\n",
       "         2.7843e-01, -1.4767e-01, -5.5677e-01,  1.4658e-01, -9.5095e-03,\n",
       "         1.1658e-02,  1.0204e-01, -1.2792e-01, -8.4430e-01, -1.2181e-01,\n",
       "        -1.6801e-02, -3.3279e-01, -1.5520e-01, -2.3131e-01, -1.9181e-01,\n",
       "        -1.8823e+00, -7.6746e-01,  9.9051e-02, -4.2125e-01, -1.9526e-01,\n",
       "         4.0071e+00, -1.8594e-01, -5.2287e-01, -3.1681e-01,  5.9213e-04,\n",
       "         7.4449e-03,  1.7778e-01, -1.5897e-01,  1.2041e-02, -5.4223e-02,\n",
       "        -2.9871e-01, -1.5749e-01, -3.4758e-01, -4.5637e-02, -4.4251e-01,\n",
       "         1.8785e-01,  2.7849e-03, -1.8411e-01, -1.1514e-01, -7.8581e-01])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model = next(iter(embeddings.values())).size()[0]\n",
    "d_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read all the words in GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_words = []\n",
    "glove = []\n",
    "for word, vector in embeddings.items():\n",
    "    glove_words += [word]\n",
    "    glove += [vector]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', ',', '.', 'of', 'to', 'and', 'in', 'a', '\"', \"'s\"]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we create a tensor with the vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = torch.stack(glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4.1800e-01,  2.4968e-01, -4.1242e-01,  1.2170e-01,  3.4527e-01,\n",
       "        -4.4457e-02, -4.9688e-01, -1.7862e-01, -6.6023e-04, -6.5660e-01,\n",
       "         2.7843e-01, -1.4767e-01, -5.5677e-01,  1.4658e-01, -9.5095e-03,\n",
       "         1.1658e-02,  1.0204e-01, -1.2792e-01, -8.4430e-01, -1.2181e-01,\n",
       "        -1.6801e-02, -3.3279e-01, -1.5520e-01, -2.3131e-01, -1.9181e-01,\n",
       "        -1.8823e+00, -7.6746e-01,  9.9051e-02, -4.2125e-01, -1.9526e-01,\n",
       "         4.0071e+00, -1.8594e-01, -5.2287e-01, -3.1681e-01,  5.9213e-04,\n",
       "         7.4449e-03,  1.7778e-01, -1.5897e-01,  1.2041e-02, -5.4223e-02,\n",
       "        -2.9871e-01, -1.5749e-01, -3.4758e-01, -4.5637e-02, -4.4251e-01,\n",
       "         1.8785e-01,  2.7849e-03, -1.8411e-01, -1.1514e-01, -7.8581e-01])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove[0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of word embeddings (400,000) and their dimension (50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([400000, 50])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model = glove.size()[1]\n",
    "d_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reserve three special symbols: padding, unknown, and the first classification token of BERT. See the lecture on transformers for a clarification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "specials = ['[PAD]', '[UNK]', '[CLS]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[PAD]', '[UNK]', '[CLS]', 'the', ',', '.', 'of', 'to', 'and', 'in']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_words = specials + glove_words\n",
    "glove_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add the vectors for the special tokens to our tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = torch.vstack((torch.zeros((3, d_model)), glove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([400003, 50])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0.])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4.1800e-01,  2.4968e-01, -4.1242e-01,  1.2170e-01,  3.4527e-01,\n",
       "        -4.4457e-02, -4.9688e-01, -1.7862e-01, -6.6023e-04, -6.5660e-01,\n",
       "         2.7843e-01, -1.4767e-01, -5.5677e-01,  1.4658e-01, -9.5095e-03,\n",
       "         1.1658e-02,  1.0204e-01, -1.2792e-01, -8.4430e-01, -1.2181e-01,\n",
       "        -1.6801e-02, -3.3279e-01, -1.5520e-01, -2.3131e-01, -1.9181e-01,\n",
       "        -1.8823e+00, -7.6746e-01,  9.9051e-02, -4.2125e-01, -1.9526e-01,\n",
       "         4.0071e+00, -1.8594e-01, -5.2287e-01, -3.1681e-01,  5.9213e-04,\n",
       "         7.4449e-03,  1.7778e-01, -1.5897e-01,  1.2041e-02, -5.4223e-02,\n",
       "        -2.9871e-01, -1.5749e-01, -3.4758e-01, -4.5637e-02, -4.4251e-01,\n",
       "         1.8785e-01,  2.7849e-03, -1.8411e-01, -1.1514e-01, -7.8581e-01])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove[3, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "### Tokenization\n",
    "You will now tokenize the sentences\n",
    "\n",
    "Write a regular expression that tokenizes the words, numbers, and punctuation. Use Unicode classes. Note that a punctuation is a single symbol while the words and numbers are sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "pattern = r'[\\p{L}\\d]+|\\p{P}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a tokenization function that takes a string as input and results a list of tokens. Set the string in lower case by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "def tokenize(sentence, pattern, lc=True):\n",
    "    if lc:\n",
    "        sentence = sentence.lower()\n",
    "    return list(re.findall(pattern, sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'person',\n",
       " 'on',\n",
       " 'a',\n",
       " 'horse',\n",
       " 'jumps',\n",
       " 'over',\n",
       " 'a',\n",
       " 'broken',\n",
       " 'down',\n",
       " 'airplane',\n",
       " '.']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(dataset_str[0][0], pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a code that, for each sample of your dataset, builds a triple consisting of:\n",
    "1. The first tokenized sentence, \n",
    "2. The second one, and \n",
    "3. The class\n",
    "\n",
    "Build a list of all these triples to represent your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "dataset_tokens = []\n",
    "for entry in dataset_str:\n",
    "    dataset_tokens += [(tokenize(entry[0], pattern, lc=True), tokenize(entry[1], pattern, lc=True), entry[2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['a',\n",
       "  'person',\n",
       "  'on',\n",
       "  'a',\n",
       "  'horse',\n",
       "  'jumps',\n",
       "  'over',\n",
       "  'a',\n",
       "  'broken',\n",
       "  'down',\n",
       "  'airplane',\n",
       "  '.'],\n",
       " ['a',\n",
       "  'person',\n",
       "  'is',\n",
       "  'training',\n",
       "  'his',\n",
       "  'horse',\n",
       "  'for',\n",
       "  'a',\n",
       "  'competition',\n",
       "  '.'],\n",
       " 'neutral')"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5493"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build token-to-index `token2idx` and index-to-token `idx2token` dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "idx2token = {k: v for k, v in enumerate(glove_words)}\n",
    "token2idx = {k: v for v, k in enumerate(glove_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 10)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2idx['the'], token2idx['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('the', 'a')"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2token[3], idx2token[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect the set of all the labels (classes) from your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "labels = []\n",
    "labels=list(set([cat[2] for cat in dataset_str]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neutral', 'contradiction', 'entailment']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build label-to-index `label2idx` and index-to-label `idx2label` dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "idx2label = {k: v for k, v in enumerate(labels)}\n",
    "label2idx = {k: v for v, k in enumerate(labels)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neutral': 0, 'contradiction': 1, 'entailment': 2}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'neutral', 1: 'contradiction', 2: 'entailment'}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to convert:\n",
    "  * A list of tokens into a list of `LongTensor` indices and \n",
    "  * The class to a tensor. \n",
    "\n",
    "Your function should be able to handle two types: either a list or a string. The tokens are strored in a list and the class (label) is a string\n",
    "\n",
    "Note that an unknown token in GloVe should be mapped to the `UNK` symbol of index 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "def convert_symbols(symbols, symbol2idx):\n",
    "    ret = []\n",
    "    if type(symbols) is str:\n",
    "        if symbols not in symbol2idx.keys():\n",
    "             torch.LongTensor([1])[0]\n",
    "        else:\n",
    "            return torch.LongTensor([symbol2idx[symbols]])[0]\n",
    "    for symbol in symbols:\n",
    "        if symbol not in symbol2idx.keys():\n",
    "            ret += [1]\n",
    "        else:\n",
    "            ret += [symbol2idx[symbol]]\n",
    "    return torch.LongTensor(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert a list of tokens into a `LongTensor` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   10,   902,    16,    10,  2870, 11073,    77,    10,  2324,   138,\n",
       "         7353,     5])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_symbols(dataset_tokens[0][0], token2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  10,  902,   17,  791,   29, 2870,   13,   10,  994,    5])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_symbols(dataset_tokens[0][1], token2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neutral'"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_tokens[0][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert a label string into a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_symbols(dataset_tokens[0][2], label2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unknown tokens have the index 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'person', 'on', 'a', 'horsewww']"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize('a person on a horsewww', pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 10, 902,  16,  10,   1,   1])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_symbols(tokenize('a person on a horsewww wxwx', pattern), token2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the tokens and labels in your dataset by their indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "dataset = []\n",
    "\n",
    "for sentence1, sentence2, label in dataset_tokens:\n",
    "    dataset +=[(convert_symbols(sentence1, token2idx), convert_symbols(sentence2, token2idx),convert_symbols(label, label2idx))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   10,   902,    16,    10,  2870, 11073,    77,    10,  2324,   138,\n",
       "          7353,     5]),\n",
       " tensor([    10,    902,     17,     25,     10,  19304,      4,   7490,     32,\n",
       "         119031,      5]),\n",
       " tensor(1))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch `Embedding` class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now store the GloVe vectors in a PyTorch `Embedding` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embs = nn.Embedding(glove.size()[0],\n",
    "                          glove.size()[1],\n",
    "                          padding_idx=0).from_pretrained(glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We access the embedding for _the_ with its index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.1800e-01,  2.4968e-01, -4.1242e-01,  1.2170e-01,  3.4527e-01,\n",
       "         -4.4457e-02, -4.9688e-01, -1.7862e-01, -6.6023e-04, -6.5660e-01,\n",
       "          2.7843e-01, -1.4767e-01, -5.5677e-01,  1.4658e-01, -9.5095e-03,\n",
       "          1.1658e-02,  1.0204e-01, -1.2792e-01, -8.4430e-01, -1.2181e-01,\n",
       "         -1.6801e-02, -3.3279e-01, -1.5520e-01, -2.3131e-01, -1.9181e-01,\n",
       "         -1.8823e+00, -7.6746e-01,  9.9051e-02, -4.2125e-01, -1.9526e-01,\n",
       "          4.0071e+00, -1.8594e-01, -5.2287e-01, -3.1681e-01,  5.9213e-04,\n",
       "          7.4449e-03,  1.7778e-01, -1.5897e-01,  1.2041e-02, -5.4223e-02,\n",
       "         -2.9871e-01, -1.5749e-01, -3.4758e-01, -4.5637e-02, -4.4251e-01,\n",
       "          1.8785e-01,  2.7849e-03, -1.8411e-01, -1.1514e-01, -7.8581e-01]])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_embs(torch.LongTensor([3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: Mean of GloVe Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('A person on a horse jumps over a broken down airplane.',\n",
       " 'A person is training his horse for a competition.',\n",
       " 'neutral')"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1_str = dataset_str[0][0]\n",
    "s2_str = dataset_str[0][1]\n",
    "s3_str = dataset_str[0][2]\n",
    "s1_str, s2_str, s3_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   10,   902,    16,    10,  2870, 11073,    77,    10,  2324,   138,\n",
       "          7353,     5]),\n",
       " tensor([  10,  902,   17,  791,   29, 2870,   13,   10,  994,    5]),\n",
       " tensor(0))"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1_idx = dataset[0][0]\n",
    "s2_idx = dataset[0][1]\n",
    "s3_idx = dataset[0][2]\n",
    "s1_idx, s2_idx, s3_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that takes a list of indices and the GloVe embeddings as input and that computes the mean of the word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def mean_embs(input_idx: torch.LongTensor, glove_embs: nn.Embedding) -> torch.tensor:\n",
    "    return torch.mean(glove_embs(input_idx), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1988,  0.1043,  0.0180, -0.0851,  0.5251,  0.4551, -0.4729, -0.0604,\n",
      "         0.1335, -0.1824, -0.1023, -0.2145, -0.3953,  0.3100,  0.3126, -0.1235,\n",
      "        -0.1631,  0.1271, -0.8334, -0.5111,  0.0911,  0.1766, -0.1190, -0.1795,\n",
      "         0.2117, -1.6935, -0.1754,  0.3900,  0.4590, -0.1137,  3.0905, -0.0358,\n",
      "        -0.2404,  0.2918,  0.1015, -0.0099,  0.2168,  0.1239,  0.1565, -0.2061,\n",
      "        -0.1449,  0.0871, -0.1085,  0.1992, -0.0306, -0.2125,  0.1155, -0.3489,\n",
      "         0.2139, -0.1993])\n"
     ]
    }
   ],
   "source": [
    "mean_embs(dataset[0][0], glove_embs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to compute the cosine of two vectors. You will return `torch.tensor(0.0)` if one of the vectors is zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def compute_cosine(v1: torch.tensor, v2: torch.tensor) -> torch.tensor:\n",
    "    if torch.all(v1 == 0) or torch.all(v2 == 0):\n",
    "        return torch.tensor(0.0)\n",
    "    return torch.cosine_similarity(v1,v2, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.1988,  0.1043,  0.0180, -0.0851,  0.5251,  0.4551, -0.4729, -0.0604,\n",
       "          0.1335, -0.1824, -0.1023, -0.2145, -0.3953,  0.3100,  0.3126, -0.1235,\n",
       "         -0.1631,  0.1271, -0.8334, -0.5111,  0.0911,  0.1766, -0.1190, -0.1795,\n",
       "          0.2117, -1.6935, -0.1754,  0.3900,  0.4590, -0.1137,  3.0905, -0.0358,\n",
       "         -0.2404,  0.2918,  0.1015, -0.0099,  0.2168,  0.1239,  0.1565, -0.2061,\n",
       "         -0.1449,  0.0871, -0.1085,  0.1992, -0.0306, -0.2125,  0.1155, -0.3489,\n",
       "          0.2139, -0.1993]),\n",
       " tensor([ 1.0878e-01,  3.7231e-01, -4.7114e-01, -1.3591e-02,  5.1340e-01,\n",
       "          3.7193e-01, -4.4592e-01, -5.9900e-02,  2.5352e-01, -1.3076e-01,\n",
       "          1.6231e-01,  2.3146e-03, -3.6474e-01,  2.3840e-03,  3.4653e-01,\n",
       "         -2.1769e-01,  2.5946e-02,  3.9537e-01, -6.9517e-01, -3.4811e-01,\n",
       "         -4.9454e-02,  1.4977e-01, -1.2447e-01,  8.1851e-02,  6.2581e-02,\n",
       "         -1.8692e+00, -3.1502e-01, -7.4079e-02,  1.2478e-01, -1.6717e-02,\n",
       "          3.3707e+00,  8.7725e-02, -4.0180e-01, -1.8131e-01,  2.5315e-01,\n",
       "          1.7589e-01,  2.2877e-01,  4.3286e-01, -1.6315e-02, -3.6988e-01,\n",
       "         -2.8208e-02,  3.1538e-02, -2.4721e-01,  2.5963e-01,  1.3792e-02,\n",
       "         -2.6431e-01,  1.7081e-02, -2.0042e-01,  1.6257e-01,  2.1471e-01]))"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = mean_embs(s1_idx, glove_embs)\n",
    "v2 = mean_embs(s2_idx, glove_embs)\n",
    "v1, v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9426)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_cosine(v1, v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compute the cosine of pairs for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5493/5493 [00:02<00:00, 2528.22it/s]\n"
     ]
    }
   ],
   "source": [
    "cos_sim = {'entailment': 0.0, 'neutral': 0.0, 'contradiction': 0.0}\n",
    "cnt = {'entailment': 0.0, 'neutral': 0.0, 'contradiction': 0.0}\n",
    "for data in tqdm(dataset):\n",
    "    cos_val = compute_cosine(\n",
    "        mean_embs(data[0], glove_embs),\n",
    "        mean_embs(data[1], glove_embs))\n",
    "    class_name = idx2label[data[2].item()]\n",
    "    cos_sim[class_name] += cos_val\n",
    "    cnt[class_name] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will comment these values in your report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entailment': tensor(0.9453),\n",
       " 'neutral': tensor(0.9385),\n",
       " 'contradiction': tensor(0.9298)}"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for key in cos_sim.keys():\n",
    "    cos_sim[key] /= cnt[key]\n",
    "cos_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SBERT: The Stacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now create the SBERT architecture and replicate the pipeline in Fig. 1 in the paper. In the next cells, we walk through the figure from the bottom to the top."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first layer builds an input consisting of two sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   3,  360, 5453]), tensor([   3,  194,  368, 2929]))"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1 = torch.LongTensor(\n",
    "    list(map(lambda x: token2idx.get(x, 1), tokenize('the small cat', pattern))))\n",
    "p2 = torch.LongTensor(\n",
    "    list(map(lambda x: token2idx.get(x, 1), tokenize('the very big dog', pattern))))\n",
    "p1, p2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we have the BERT layer. Using PyTorch classes, create an encoder of four layers where each layer has five heads. You will use the classes `TransformerEncoderLayer` and `TransformerEncoder`. The dimensionality `d_model` is 50 as this is the size of GloVe vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaiser/nlp/edan20/.venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "# Write your code\n",
    "d_model = 50\n",
    "n_heads = 5\n",
    "n_layers = 4\n",
    "encoder_layer = nn.TransformerEncoderLayer(d_model, n_heads)\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, n_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerEncoder(\n",
       "  (layers): ModuleList(\n",
       "    (0-3): 4 x TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=50, out_features=50, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=50, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=50, bias=True)\n",
       "      (norm1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now a BERT encoder. We associate each input index to an embedding. In the next cell, we create three embedding vectors correponding to three words.\n",
    "\n",
    "In the rest of the program, all our batches will have only one sample to eliminate the need for padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8513, 0.8694, 0.6493, 0.6627, 0.1890, 0.1669, 0.6102, 0.4930, 0.9393,\n",
       "         0.5985, 0.8426, 0.7923, 0.4050, 0.0313, 0.8659, 0.1652, 0.7836, 0.9420,\n",
       "         0.3378, 0.0835, 0.6805, 0.6599, 0.7762, 0.1035, 0.1236, 0.9148, 0.9980,\n",
       "         0.1865, 0.4521, 0.6066, 0.2818, 0.2433, 0.9913, 0.2442, 0.3687, 0.5091,\n",
       "         0.0468, 0.2367, 0.1093, 0.5191, 0.5378, 0.4536, 0.6149, 0.9417, 0.8473,\n",
       "         0.4202, 0.9277, 0.4080, 0.6110, 0.4385],\n",
       "        [0.8148, 0.0889, 0.1109, 0.2051, 0.6071, 0.2103, 0.8786, 0.1547, 0.7483,\n",
       "         0.2792, 0.0326, 0.0607, 0.4083, 0.7817, 0.4170, 0.6004, 0.0905, 0.8027,\n",
       "         0.5779, 0.6696, 0.5908, 0.1181, 0.4487, 0.0164, 0.1517, 0.7609, 0.4741,\n",
       "         0.5662, 0.0411, 0.9370, 0.7367, 0.1057, 0.1327, 0.3362, 0.3389, 0.9873,\n",
       "         0.1246, 0.2141, 0.2215, 0.4783, 0.1156, 0.6873, 0.3990, 0.7401, 0.7957,\n",
       "         0.8729, 0.3142, 0.7053, 0.0058, 0.8370],\n",
       "        [0.9663, 0.0801, 0.0089, 0.0195, 0.6630, 0.3194, 0.2528, 0.3010, 0.5041,\n",
       "         0.2434, 0.2308, 0.7517, 0.7200, 0.4308, 0.3975, 0.1290, 0.5330, 0.1225,\n",
       "         0.9532, 0.8239, 0.8446, 0.5316, 0.4406, 0.5783, 0.1637, 0.7978, 0.8958,\n",
       "         0.0995, 0.2298, 0.0791, 0.8202, 0.4496, 0.2635, 0.2033, 0.9181, 0.7194,\n",
       "         0.1476, 0.9311, 0.6016, 0.3718, 0.2563, 0.7107, 0.2646, 0.8160, 0.0844,\n",
       "         0.1458, 0.3874, 0.1581, 0.3255, 0.9243]])"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src = torch.rand(3, d_model)\n",
    "src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pass it to the encoder to encode the input into three vectors of the same size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1561,  0.9270, -0.0531, -0.1262, -1.6270, -0.8453,  1.1698,  1.0318,\n",
       "          0.5854,  0.1817,  0.2220,  1.2739, -0.0718, -1.6325,  0.4267, -1.4168,\n",
       "          1.6651,  1.9181, -0.4095, -1.5274, -0.6302, -0.0255,  1.1792, -1.1258,\n",
       "          0.1369,  0.2707, -0.3949, -0.7383,  0.5935, -1.3385,  0.4781,  0.5515,\n",
       "          0.4852, -0.9750, -0.7225,  1.0087, -1.1806, -1.8109, -0.7859, -0.9557,\n",
       "         -0.5098, -0.8188,  2.2637,  1.4683,  0.5490, -1.1618,  0.0869,  0.6264,\n",
       "          0.6433, -0.0150],\n",
       "        [ 1.0555, -0.8835, -0.6203, -1.3213, -0.8645, -1.5497,  1.9115,  0.9205,\n",
       "          1.5206, -1.9090, -1.5796, -0.2205,  0.5866, -0.1754, -1.5379,  1.3430,\n",
       "         -0.1595,  1.9807,  0.2282, -0.3286,  0.1910,  0.7095,  1.3777, -0.6529,\n",
       "          1.2557, -0.0513, -0.7655,  0.4107, -0.5531,  0.6369,  0.5534,  0.1690,\n",
       "         -0.6434, -0.3949, -0.9959,  1.1335, -0.9955, -0.3546,  0.4247, -0.1069,\n",
       "         -1.3875, -0.4795,  1.4601, -0.0709,  0.5048,  0.1904, -0.2101,  0.2384,\n",
       "         -1.7402,  1.7492],\n",
       "        [ 2.3985, -0.8328, -0.7830, -1.7953, -0.6336, -0.5434,  0.1199, -0.2784,\n",
       "         -0.0442, -0.9714, -0.9197,  1.7432,  0.4782, -0.3523, -1.0523, -0.3037,\n",
       "          0.9093,  1.5896,  1.7537,  0.6891,  0.2588,  0.8664,  1.0533,  0.3234,\n",
       "         -0.3399,  0.4861, -0.2750, -1.7269,  0.3014, -1.6681,  0.2030,  0.4481,\n",
       "         -0.3823,  0.4095,  0.6393, -0.3315, -1.2175,  0.0926,  1.0321, -2.2894,\n",
       "          0.0406,  0.5811,  1.5974,  0.8024,  0.4956, -1.8593, -0.8102,  0.1934,\n",
       "         -0.5578,  0.4621]], grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = transformer_encoder(src)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a statement that will compute the mean of these embeddings. You will use the `mean(dim)` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.5367, -0.2631, -0.4855, -1.0809, -1.0417, -0.9794,  1.0670,  0.5580,\n",
       "         0.6872, -0.8996, -0.7591,  0.9322,  0.3310, -0.7200, -0.7212, -0.1258,\n",
       "         0.8050,  1.8295,  0.5242, -0.3889, -0.0601,  0.5168,  1.2034, -0.4851,\n",
       "         0.3509,  0.2352, -0.4784, -0.6848,  0.1139, -0.7899,  0.4115,  0.3896,\n",
       "        -0.1802, -0.3201, -0.3597,  0.6036, -1.1312, -0.6910,  0.2236, -1.1173,\n",
       "        -0.6189, -0.2390,  1.7737,  0.7333,  0.5165, -0.9435, -0.3111,  0.3527,\n",
       "        -0.5516,  0.7321], grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write your code\n",
    "out.mean(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have understood the first steps of SBERT, we can implement them in a class.\n",
    "\n",
    "In the next cell, write the `forward()` method that takes the two sentences as input in the form of two `LongTensor` of indices.\n",
    "1. Extract their embeddings from the GloVe embedding matrix.\n",
    "2. Encode them with the transformer, and \n",
    "3. Compute their respective mean. You will call these vectors $\\mathbf{u}$ and $\\mathbf{v}$\n",
    "4. Return these two vectors of means. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the forward method\n",
    "class SBERT(nn.Module):\n",
    "    def __init__(self, nbr_classes, glove, d_model=50, nhead=5, num_layers=4):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embeddings = nn.Embedding(\n",
    "            glove.size()[0],\n",
    "            glove.size()[1],\n",
    "            padding_idx=0).from_pretrained(glove)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model, nhead)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            self.encoder_layer, num_layers=num_layers)\n",
    "        # We do not use this last line for now\n",
    "        self.fc = nn.Linear(3 * d_model, nbr_classes)\n",
    "\n",
    "    def forward(self, u, v):\n",
    "        u = self.transformer_encoder(self.embeddings(u)).mean(0)\n",
    "        v = self.transformer_encoder(self.embeddings(v)).mean(0)\n",
    "        return u, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaiser/nlp/edan20/.venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "sbert = SBERT(3, glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SBERT(\n",
       "  (embeddings): Embedding(400003, 50)\n",
       "  (encoder_layer): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=50, out_features=50, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=50, out_features=2048, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=2048, out_features=50, bias=True)\n",
       "    (norm1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=50, out_features=50, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=50, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=50, bias=True)\n",
       "        (norm1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=150, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbert.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   10,   902,    16,    10,  2870, 11073,    77,    10,  2324,   138,\n",
       "          7353,     5]),\n",
       " tensor([  10,  902,   17,  791,   29, 2870,   13,   10,  994,    5]),\n",
       " tensor(0))"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, v = sbert(dataset[0][0], dataset[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.3985, -0.1307, -0.1265, -1.0439,  1.6861,  0.2679, -1.4272,  1.6145,\n",
       "         -0.2086, -0.9649,  0.9879, -0.7341,  0.2149,  0.5277, -0.6891,  0.4997,\n",
       "         -0.1314,  0.1607, -1.6328, -2.3853,  0.0715,  0.7381,  0.3569, -0.6683,\n",
       "          0.9140, -1.2008, -0.1234, -0.1603,  2.1081,  0.0247,  1.6057, -0.4917,\n",
       "          0.5822, -0.3095,  1.4816, -0.7752, -0.1061, -0.5838,  0.9331, -1.6054,\n",
       "          0.7096,  0.5726, -0.2435,  0.0568,  1.0256, -0.6912,  1.3511, -1.4078,\n",
       "          0.4954, -0.7460], grad_fn=<MeanBackward1>),\n",
       " tensor([-7.0530e-01, -7.0165e-02, -4.4094e-01, -1.3735e+00,  1.8510e+00,\n",
       "          3.3963e-01, -1.3278e+00,  1.5353e+00,  5.2875e-01, -1.1960e+00,\n",
       "          6.9692e-01, -5.7449e-01,  2.8840e-02,  6.8757e-01, -1.1996e+00,\n",
       "          1.0285e+00, -1.5438e-01,  1.7708e-02, -1.3778e+00, -1.6271e+00,\n",
       "          9.8852e-02,  6.4114e-01, -1.9679e-02, -2.1790e-01,  9.8381e-01,\n",
       "         -1.3544e+00, -6.1728e-01, -7.2833e-01,  1.9349e+00, -2.5259e-02,\n",
       "          1.5741e+00, -5.5559e-01,  7.9025e-01, -6.4536e-01,  1.3810e+00,\n",
       "         -4.0871e-01, -4.1269e-01, -3.5259e-01,  4.8751e-01, -1.8508e+00,\n",
       "          6.8825e-01,  8.5427e-01, -4.6222e-01, -9.1861e-04,  1.0845e+00,\n",
       "         -6.0998e-01,  1.7913e+00, -1.1644e+00,  6.9751e-01, -2.4840e-01],\n",
       "        grad_fn=<MeanBackward1>))"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u, v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the code that concatenate `u`, `v`, and `|u-v|`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-3.9850e-01, -1.3071e-01, -1.2646e-01, -1.0439e+00,  1.6861e+00,\n",
       "         2.6788e-01, -1.4272e+00,  1.6145e+00, -2.0860e-01, -9.6490e-01,\n",
       "         9.8787e-01, -7.3408e-01,  2.1491e-01,  5.2770e-01, -6.8911e-01,\n",
       "         4.9974e-01, -1.3141e-01,  1.6068e-01, -1.6328e+00, -2.3853e+00,\n",
       "         7.1535e-02,  7.3806e-01,  3.5695e-01, -6.6835e-01,  9.1402e-01,\n",
       "        -1.2008e+00, -1.2344e-01, -1.6031e-01,  2.1081e+00,  2.4748e-02,\n",
       "         1.6057e+00, -4.9172e-01,  5.8216e-01, -3.0946e-01,  1.4816e+00,\n",
       "        -7.7518e-01, -1.0614e-01, -5.8382e-01,  9.3307e-01, -1.6054e+00,\n",
       "         7.0960e-01,  5.7258e-01, -2.4351e-01,  5.6770e-02,  1.0256e+00,\n",
       "        -6.9119e-01,  1.3511e+00, -1.4078e+00,  4.9536e-01, -7.4602e-01,\n",
       "        -7.0530e-01, -7.0165e-02, -4.4094e-01, -1.3735e+00,  1.8510e+00,\n",
       "         3.3963e-01, -1.3278e+00,  1.5353e+00,  5.2875e-01, -1.1960e+00,\n",
       "         6.9692e-01, -5.7449e-01,  2.8840e-02,  6.8757e-01, -1.1996e+00,\n",
       "         1.0285e+00, -1.5438e-01,  1.7708e-02, -1.3778e+00, -1.6271e+00,\n",
       "         9.8852e-02,  6.4114e-01, -1.9679e-02, -2.1790e-01,  9.8381e-01,\n",
       "        -1.3544e+00, -6.1728e-01, -7.2833e-01,  1.9349e+00, -2.5259e-02,\n",
       "         1.5741e+00, -5.5559e-01,  7.9025e-01, -6.4536e-01,  1.3810e+00,\n",
       "        -4.0871e-01, -4.1269e-01, -3.5259e-01,  4.8751e-01, -1.8508e+00,\n",
       "         6.8825e-01,  8.5427e-01, -4.6222e-01, -9.1861e-04,  1.0845e+00,\n",
       "        -6.0998e-01,  1.7913e+00, -1.1644e+00,  6.9751e-01, -2.4840e-01,\n",
       "         3.0680e-01,  6.0545e-02,  3.1448e-01,  3.2958e-01,  1.6492e-01,\n",
       "         7.1751e-02,  9.9393e-02,  7.9222e-02,  7.3735e-01,  2.3112e-01,\n",
       "         2.9096e-01,  1.5960e-01,  1.8607e-01,  1.5987e-01,  5.1044e-01,\n",
       "         5.2872e-01,  2.2978e-02,  1.4297e-01,  2.5504e-01,  7.5819e-01,\n",
       "         2.7317e-02,  9.6918e-02,  3.7662e-01,  4.5045e-01,  6.9789e-02,\n",
       "         1.5359e-01,  4.9384e-01,  5.6802e-01,  1.7315e-01,  5.0007e-02,\n",
       "         3.1614e-02,  6.3872e-02,  2.0809e-01,  3.3590e-01,  1.0067e-01,\n",
       "         3.6647e-01,  3.0655e-01,  2.3123e-01,  4.4556e-01,  2.4544e-01,\n",
       "         2.1352e-02,  2.8169e-01,  2.1871e-01,  5.7688e-02,  5.8991e-02,\n",
       "         8.1209e-02,  4.4026e-01,  2.4346e-01,  2.0215e-01,  4.9763e-01],\n",
       "       grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write your code\n",
    "torch.concat((u,v, torch.abs(u-v)), dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to add the logistic regression head. Complement the `forward()` method so that it concatenates, $\\mathbf{u}$, $\\mathbf{v}$, and $\\mathbf{|u - v|}$ and outputs three classes. You have only two lines to add."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the forward() method\n",
    "class SBERT(nn.Module):\n",
    "    def __init__(self, nbr_classes, glove, d_model=50, nhead=5, num_layers=4):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embeddings = nn.Embedding(glove.size()[0],\n",
    "                                       glove.size()[1],\n",
    "                                       padding_idx=0).from_pretrained(glove)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model, nhead)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            self.encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(3 * d_model, nbr_classes)\n",
    "\n",
    "    def forward(self, u, v):\n",
    "        u = self.transformer_encoder(self.embeddings(u)).mean(0)  # Doesn't count\n",
    "        v = self.transformer_encoder(self.embeddings(v)).mean(0)  # Doesn't count\n",
    "        uvabs = torch.abs(u-v)  # Line 1\n",
    "        x = torch.concat((u,v,uvabs), dim=0)  # Line 2\n",
    "        return self.fc(x)  # Pre-existing line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaiser/nlp/edan20/.venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "sbert = SBERT(3, glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SBERT(\n",
       "  (embeddings): Embedding(400003, 50)\n",
       "  (encoder_layer): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=50, out_features=50, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=50, out_features=2048, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=2048, out_features=50, bias=True)\n",
       "    (norm1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=50, out_features=50, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=50, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=50, bias=True)\n",
       "        (norm1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=150, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbert.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   10,   902,    16,    10,  2870, 11073,    77,    10,  2324,   138,\n",
       "          7353,     5]),\n",
       " tensor([  10,  902,   17,  791,   29, 2870,   13,   10,  994,    5]),\n",
       " tensor(0))"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the complete model and we have three outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1996, -0.1967, -0.8664], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbert(dataset[0][0], dataset[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training SBERT\n",
    "We now train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SBERT(\n",
       "  (embeddings): Embedding(400003, 50)\n",
       "  (encoder_layer): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=50, out_features=50, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=50, out_features=2048, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=2048, out_features=50, bias=True)\n",
       "    (norm1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=50, out_features=50, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=50, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=50, bias=True)\n",
       "        (norm1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=150, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbert.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()    # cross entropy loss\n",
    "optimizer = torch.optim.Adam(sbert.parameters(), lr=0.00002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   10,   902,    16,    10,  2870, 11073,    77,    10,  2324,   138,\n",
       "          7353,     5]),\n",
       " tensor([  10,  902,   17,  791,   29, 2870,   13,   10,  994,    5]),\n",
       " tensor(2))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1050, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred = sbert(dataset[0][0],dataset[0][1])\n",
    "loss_fn(ypred, dataset[0][2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the training loop. You will process one sample at a time to simplify it i.e. no batch. Record the training loss.\n",
    "\n",
    "A better design would use a `Dataset` object. We will see this construct in the last laboratory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5493/5493 [02:46<00:00, 33.01it/s]\n",
      "100%|██████████| 5493/5493 [02:48<00:00, 32.69it/s]\n",
      "100%|██████████| 5493/5493 [03:20<00:00, 27.41it/s]\n",
      "100%|██████████| 5493/5493 [02:51<00:00, 32.10it/s]\n",
      "100%|██████████| 5493/5493 [03:11<00:00, 28.63it/s]\n"
     ]
    }
   ],
   "source": [
    "# Write your code\n",
    "ce_train_loss = []\n",
    "for epoch in range(5):\n",
    "    loss_train = 0\n",
    "    for entry in tqdm(dataset):\n",
    "        y_prediction = sbert(entry[0], entry[1])\n",
    "        loss = loss_fn(y_prediction, entry[2])\n",
    "        loss_train += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    ce_train_loss += [loss_train/ len(dataset)]\n",
    "    print(ce_train_loss[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "ce_train_loss = []\n",
    "for epoch in range(5):\n",
    "    loss_train = 0\n",
    "    for entry in tqdm(dataset):\n",
    "        y_prediction = sbert(entry[0], entry[1])\n",
    "        loss = loss_fn(y_prediction, entry[2])\n",
    "        loss_train += loss.item()\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(loss_train/ len(dataset))\n",
    "    ce_train_loss += [loss_train/ len(dataset)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0205019292540831,\n",
       " 0.9647003018680262,\n",
       " 0.9266434221569332,\n",
       " 0.8990261422705205,\n",
       " 0.8710259519315322]"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ce_train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(range(len(ce_train_loss)),\n",
    "            ce_train_loss, c='b', marker='x')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Cross entropy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(sbert, 'sbert_mini.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_saved = torch.load('sbert_mini.pt', weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  10,  902,   17,  791,   29, 2870,   13,   10,  994,    5])"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.9413, -0.8957, -0.6693], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbert(dataset[0][0], dataset[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write a function to apply the model to a sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sbert(model, token_indices):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        v = model.embeddings(token_indices)\n",
    "        v = model.transformer_encoder(v).mean(dim=0)\n",
    "        return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.3527, -0.2442, -0.9457, -0.1529,  0.8396,  0.0924,  0.5199, -0.6223,\n",
       "         0.2678,  0.2673, -0.5649,  0.4914,  1.3380,  0.6183,  0.3538, -0.8290,\n",
       "        -0.3041, -0.8285, -0.1398,  0.5707, -3.2667, -0.3211, -0.0548,  1.5404,\n",
       "         0.0282, -0.2668,  0.2708,  0.1624, -0.4569,  0.4539,  1.7463,  0.5589,\n",
       "        -0.2118, -0.8895,  0.7014,  0.7720,  0.1179, -0.2493, -1.0591, -0.0130,\n",
       "        -0.3772, -1.0852, -0.4685,  0.4160,  1.3157, -0.4737,  0.5383,  0.1035,\n",
       "        -0.4348, -0.2849])"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_sbert(sbert, dataset[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'neutral', 1: 'contradiction', 2: 'entailment'}"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the model to all our pairs and, for each pair, we compute the cosine of the resulting embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5493/5493 [00:41<00:00, 133.74it/s]\n"
     ]
    }
   ],
   "source": [
    "cos_sim = {'entailment': 0.0, 'neutral': 0.0, 'contradiction': 0.0}\n",
    "cnt = {'entailment': 0.0, 'neutral': 0.0, 'contradiction': 0.0}\n",
    "sbert.eval()\n",
    "\n",
    "for data in tqdm(dataset):\n",
    "    cos_val = compute_cosine(\n",
    "        encode_sbert(sbert, data[0]),\n",
    "        encode_sbert(sbert, data[1]))\n",
    "    cos_sim[idx2label[data[2].item()]] += cos_val\n",
    "    cnt[idx2label[data[2].item()]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'entailment': tensor(1215.8177),\n",
       "  'neutral': tensor(872.9430),\n",
       "  'contradiction': tensor(717.8903)},\n",
       " {'entailment': 1839.0, 'neutral': 1821.0, 'contradiction': 1833.0})"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim, cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entailment': tensor(0.6611),\n",
       " 'neutral': tensor(0.4794),\n",
       " 'contradiction': tensor(0.3916)}"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for key in cos_sim.keys():\n",
    "    cos_sim[key] /= cnt[key]\n",
    "cos_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the Embedder to Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"The weather is lovely today.\",\n",
    "    \"It's so sunny outside!\",\n",
    "    \"He drove to the stadium.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the', 'weather', 'is', 'lovely', 'today', '.'],\n",
       " ['it', \"'\", 's', 'so', 'sunny', 'outside', '!'],\n",
       " ['he', 'drove', 'to', 'the', 'stadium', '.']]"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sents = [tokenize(sent, pattern) for sent in sentences]\n",
    "tokenized_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([    3,  1623,    17, 11130,   376,     5]),\n",
       " tensor([  23,   60, 1537,  103, 9328,  590,  808]),\n",
       " tensor([  21, 3189,    7,    3, 1355,    5])]"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexed_sents = [convert_symbols(sent, token2idx) for sent in tokenized_sents]\n",
    "indexed_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([ 0.5592,  0.0377, -0.9360,  0.3906,  1.3718, -0.3215, -1.1960, -0.6279,\n",
       "          1.2481,  0.3187, -1.0204,  0.7419,  1.6484,  0.0124,  0.3552, -1.3254,\n",
       "          0.4078, -0.4925, -0.3692,  0.2470, -2.7276, -1.5478,  0.5812,  0.1300,\n",
       "          0.9045, -0.0064, -0.4089,  1.4055, -0.2675,  0.1694,  1.4529,  0.2766,\n",
       "          0.4295, -0.1499,  0.5604,  0.8865, -0.5173, -0.6520, -1.4015,  0.4084,\n",
       "         -1.7421,  0.2239,  0.5700,  0.2717,  0.7856,  0.5811,  0.3752, -0.4225,\n",
       "         -0.6274, -0.7040]),\n",
       " tensor([ 0.6747, -0.6114, -0.3522,  0.7576,  0.8220, -0.3715,  0.1101, -0.3327,\n",
       "          0.7650,  0.3705, -0.9431,  0.9169,  0.6894,  0.3382,  0.0912, -1.2078,\n",
       "          0.9909,  0.3047, -1.6038,  0.2528, -0.6126, -1.4955,  0.3125, -0.7395,\n",
       "          1.3092, -0.1427, -0.3832,  2.2641, -0.0805,  0.0863,  0.1531,  0.1047,\n",
       "          0.7599,  0.9504,  0.3587,  0.1069, -1.2812, -0.5631, -0.7115,  0.4947,\n",
       "         -1.6822, -0.6343,  0.4600, -0.2827, -0.5189, -0.0415,  0.3258, -0.8115,\n",
       "          0.0446,  0.5111]),\n",
       " tensor([ 0.7338, -1.6042, -0.1345,  1.1312,  0.9531, -0.4885,  0.8159, -0.8379,\n",
       "         -0.9974, -0.5039, -1.0478,  0.2910,  1.3286,  0.6581,  0.0516, -1.6375,\n",
       "          0.4322,  0.3525, -0.6703,  0.5919, -2.2394, -1.0109,  0.4052,  0.1730,\n",
       "          0.1741, -0.9327,  0.2175,  1.1043,  0.0154,  0.8266,  1.0562,  0.5282,\n",
       "          0.9539,  0.5809,  1.0839,  1.0387, -1.2137,  0.5885,  0.5557,  1.3534,\n",
       "         -0.7286, -1.5943,  0.5093, -0.2213, -0.4191, -0.3961,  0.0810, -0.6822,\n",
       "         -1.0695, -0.2670])]"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_sents = [encode_sbert(sbert, sent) for sent in indexed_sents]\n",
    "encoded_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.6697, 0.5573],\n",
       "        [0.6697, 1.0000, 0.6233],\n",
       "        [0.5573, 0.6233, 1.0000]])"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([compute_cosine(s1, s2)\n",
    "              for s1 in encoded_sents\n",
    "              for s2 in encoded_sents]).reshape(len(sentences), len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning in your assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now your are done with the program. To complete this assignment, you will write a report where you will:\n",
    "1. Write a short individual report on your program. I recommend that you use this structure for your report:\n",
    "      1. Objectives and dataset\n",
    "      2. Method and program structure, where you should outline your program and possibly describe difficult parts.\n",
    "      3. Results.\n",
    "      4. Conclusion.\n",
    "2. In Sect. _Method and program structure_, do not forget to:\n",
    "   * Summarize the baseline\n",
    "   * Summarize SBERT\n",
    "\n",
    "The whole report should be of 2 to 3 pages.\n",
    "\n",
    "Submit your report as well as your **notebook** (for archiving purposes) to Canvas: https://canvas.education.lu.se/. To write your report, use Latex. This will probably help you structure your text. You can use the Overleaf online editor (www.overleaf.com). You will then upload a PDF file in Canvas.\n",
    "\n",
    "The submission deadline is October 18, 2024."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
